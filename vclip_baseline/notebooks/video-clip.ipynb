{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torchvision\n",
    "# from skimage import io\n",
    "import cv2\n",
    "\n",
    "import copy, json, pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "sns.set_style('whitegrid')\n",
    "from mmpt.models import MMPTModel\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "options = ['0', '1', '2', '3', '4', '5', 'yes', 'no', 'rubber', 'metal', 'sphere', 'cube', 'cylinder', 'gray', 'brown', 'green', 'red', 'blue', 'purple', 'yellow', 'cyan']\n",
    "option_id_map = {\n",
    "    o:i for i,o in enumerate(options)\n",
    "}\n",
    "id_option_map = {\n",
    "    i:o for i,o in enumerate(options)\n",
    "}\n",
    "task_heads = ['descriptive', 'explanatory', 'predictive', 'counterfactual']\n",
    "binary_id_map = {'wrong': 0, 'correct': 1}\n",
    "id_binary_map = {0: 'wrong', 1: 'correct'}\n",
    "ques_dict_keys = [\"caps\", \"cmasks\", \"q_ids\", \"choice_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessQuestions:\n",
    "    def __init__(self, tokenizer, aligner):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.aligner = aligner\n",
    "        pass\n",
    "        \n",
    "    def get_qa_batch(self, ques_list):\n",
    "        #TODO: get qa batches for the current task_head\n",
    "        '''\n",
    "        INPUT:\n",
    "        ques_list: list of question_data dictionary\n",
    "        OUTPUT: question_dict, answer_dict\n",
    "        descriptive: \n",
    "            question_list: list of <question> [SEP] <question_subtype>\n",
    "            answer_list: list of respective answer as option_id_map\n",
    "        explanatory:\n",
    "            question_list: list of <question> [SEP] <choice_k>\n",
    "            answer_list: list of respective answer as binary_id_map correct = 1 / wrong = 0\n",
    "        predictive:\n",
    "            question_list: list of <question> [SEP] <choice_1 >  [SEP] <choice_2>\n",
    "            answer_list: OHE vector of respective answer as binary_id_map correct = 1 / wrong = 0\n",
    "        counterfactual:\n",
    "            question_list: list of <question> [SEP] <choice_k>\n",
    "            answer_list: list of respective answer as binary_id_map correct = 1 / wrong = 0\n",
    "        '''\n",
    "        \n",
    "        question_dict = {i:{j:[] for j in ques_dict_keys} for i in task_heads}\n",
    "        answer_dict = {i:[] for i in task_heads}\n",
    "        for j, q in enumerate(ques_list):\n",
    "            question_type = q['question_type']\n",
    "            \n",
    "            if question_type == \"descriptive\":\n",
    "                caps, cmasks = self.aligner._build_text_seq(self.tokenizer(q['question'] + \" [SEP] \" + q['question_subtype'], add_special_tokens=False)[\"input_ids\"])\n",
    "                # caps, cmasks = caps[None, :], cmasks[None, :]  # bsz=1\n",
    "                question_dict[question_type]['caps'].append(caps)\n",
    "                question_dict[question_type]['cmasks'].append(cmasks)\n",
    "\n",
    "                question_dict[question_type]['q_ids'].append(q['question_id'])\n",
    "                answer_dict[question_type].append(option_id_map[q['answer']])\n",
    "\n",
    "            elif question_type == \"explanatory\":                \n",
    "                question = q['question']\n",
    "                q_id = q['question_id']                \n",
    "                for c, choice in enumerate(q['choices']):\n",
    "                    caps, cmasks = self.aligner._build_text_seq(self.tokenizer(question + \" [SEP] \" + choice['choice'], add_special_tokens=False)[\"input_ids\"])\n",
    "                    # caps, cmasks = caps[None, :], cmasks[None, :]  # bsz=1\n",
    "                    question_dict[question_type]['caps'].append(caps)\n",
    "                    question_dict[question_type]['cmasks'].append(cmasks)\n",
    "                    \n",
    "                    question_dict[question_type]['q_ids'].append(q_id)\n",
    "                    question_dict[question_type]['choice_ids'].append(choice['choice_id'])\n",
    "                    answer_dict[question_type].append(binary_id_map[choice['answer']])\n",
    "\n",
    "            elif question_type == \"predictive\":               \n",
    "                question = q['question']\n",
    "                q_id = q['question_id']                \n",
    "                for c, choice in enumerate(q['choices']):\n",
    "                    caps, cmasks = self.aligner._build_text_seq(self.tokenizer(question + \" [SEP] \" + choice['choice'], add_special_tokens=False)[\"input_ids\"])\n",
    "                    # caps, cmasks = caps[None, :], cmasks[None, :]  # bsz=1\n",
    "                    question_dict[question_type]['caps'].append(caps)\n",
    "                    question_dict[question_type]['cmasks'].append(cmasks)\n",
    "                    \n",
    "                    question_dict[question_type]['q_ids'].append(q_id)\n",
    "                    question_dict[question_type]['choice_ids'].append(choice['choice_id'])\n",
    "                    answer_dict[question_type].append(binary_id_map[choice['answer']])\n",
    "                \n",
    "#                 question_dict[question_type].append(question + \" [SEP] \" + q['choices'][0]['choice'] + \" [SEP] \" + q['choices'][1]['choice'])               \n",
    "#                 answer_dict[question_type].append(binary_id_map[q['choices'][1]['answer']]) #  binary_id_map[q['choices'][0]['answer']], binary_id_map[q['choices'][1]['answer']]])\n",
    "\n",
    "            elif question_type == \"counterfactual\":               \n",
    "                question = q['question']\n",
    "                q_id = q['question_id']                \n",
    "                for c, choice in enumerate(q['choices']):\n",
    "                    caps, cmasks = self.aligner._build_text_seq(self.tokenizer(question + \" [SEP] \" + choice['choice'], add_special_tokens=False)[\"input_ids\"])\n",
    "                    # caps, cmasks = caps[None, :], cmasks[None, :]  # bsz=1\n",
    "                    question_dict[question_type]['caps'].append(caps)\n",
    "                    question_dict[question_type]['cmasks'].append(cmasks)\n",
    "                    \n",
    "                    question_dict[question_type]['q_ids'].append(q_id)\n",
    "                    question_dict[question_type]['choice_ids'].append(choice['choice_id'])\n",
    "                    answer_dict[question_type].append(binary_id_map[choice['answer']])\n",
    "        \n",
    "        for th in task_heads:\n",
    "            if answer_dict[th]:\n",
    "                \n",
    "                question_dict[th]['caps'] = torch.stack(question_dict[th]['caps'])\n",
    "                question_dict[th]['cmasks'] = torch.stack(question_dict[th]['cmasks'])\n",
    "                question_dict[th]['q_ids'] = torch.tensor(question_dict[th]['q_ids'], dtype=torch.long)\n",
    "                question_dict[th]['choice_ids'] = torch.tensor(question_dict[th]['choice_ids'], dtype=torch.long)\n",
    "                answer_dict[th] = torch.tensor(answer_dict[th], dtype=torch.long)\n",
    "                \n",
    "                if th != 'descriptive':\n",
    "                    answer_dict[th] = answer_dict[th].float()\n",
    "            else:\n",
    "                del question_dict[th]\n",
    "                del answer_dict[th]\n",
    "        \n",
    "        return question_dict, answer_dict\n",
    "        \n",
    "class CLEVRERDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_dir, frame_dir, tokenizer, aligner):\n",
    "        # TODO load annotations\n",
    "        assert os.path.isdir(data_dir)\n",
    "        assert os.path.isdir(frame_dir)\n",
    "        \n",
    "        with open(os.path.join(data_dir, data_dir.split(\"/\")[-1] + \".json\"), \"r\") as f:\n",
    "            self.json_data = json.load(f)\n",
    "        self.frame_dir = frame_dir\n",
    "        \n",
    "        self.process_questions = ProcessQuestions(tokenizer, aligner)\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        # get length from directory\n",
    "        return len(self.json_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "        1. Change here hardcoded path in frame_paths to os.path.join(self.frame_dir, f\"sim_{vid_id}\", \"*.png\")\n",
    "        2. Check normalization mean and std values used in image transform.\n",
    "        \"\"\"\n",
    "        \n",
    "        vid_json = self.json_data[idx]\n",
    "        vid_id = vid_json['scene_index']\n",
    "        \n",
    "        frame_dir = os.path.join(self.frame_dir, f\"sim_{vid_id:05d}\", \"*.png\")\n",
    "        frame_paths = sorted(glob(frame_dir))\n",
    "        frames = torch.stack([torchvision.io.read_image(img).float() for img in frame_paths])\n",
    "                \n",
    "        ques_dict, ans_dict = self.process_questions.get_qa_batch(vid_json['questions'])\n",
    "\n",
    "        return {'frames': frames, 'ques_dict': ques_dict, 'ans_dict': ans_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DescriptiveTaskHead(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, n_classes=21, p=0.2, input_dim=768*2):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.clf = nn.Sequential(\n",
    "\t\t\tnn.Linear(input_dim, 1024),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.Linear(1024, n_classes)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, features):\n",
    "\t\treturn self.clf(features)\n",
    "\n",
    "class ExplanatoryTaskHead(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, p=0.2, input_dim=768*2):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.clf = nn.Sequential(\n",
    "\t\t\tnn.Linear(input_dim, 1024),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.Linear(1024, 1),\n",
    "            nn.Sigmoid()\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, features):\n",
    "\t\treturn self.clf(features).reshape(-1)\n",
    "\n",
    "class PredictiveTaskHead(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, p=0.2, input_dim=768*2):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.clf = nn.Sequential(\n",
    "\t\t\tnn.Linear(input_dim, 1024),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.Linear(1024, 1),\n",
    "            nn.Sigmoid()\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, features):\n",
    "\t\treturn self.clf(features).reshape(-1)\n",
    "\n",
    "class CounterfactualTaskHead(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, p=0.2, input_dim=768*2):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.clf = nn.Sequential(\n",
    "\t\t\tnn.Linear(input_dim, 1024),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.Linear(1024, 1),\n",
    "            nn.Sigmoid()\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, features):\n",
    "\t\treturn self.clf(features).reshape(-1)\n",
    "\n",
    "class VideoCLIPModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, video_clip):\n",
    "        super().__init__()\n",
    "        self.video_clip = video_clip\n",
    "        \n",
    "        self.max_video_len = video_clip.max_video_len\n",
    "        self.video_encoder = video_clip.video_encoder\n",
    "        self.mmmodel = video_clip.model\n",
    "        \n",
    "        self.descriptive_head = DescriptiveTaskHead()\n",
    "        self.explanatory_head = ExplanatoryTaskHead()\n",
    "        self.predictive_head = PredictiveTaskHead()\n",
    "        self.counterfactual_head = CounterfactualTaskHead()\n",
    "        \n",
    "        self.head_map = {\n",
    "            'descriptive': self.descriptive_head,\n",
    "            'predictive': self.predictive_head,\n",
    "            'explanatory': self.explanatory_head,\n",
    "            'counterfactual': self.counterfactual_head\n",
    "        }\n",
    "        \n",
    "    def forward(self, example):\n",
    "        \n",
    "        N, C, H, W = example['frames'].shape\n",
    "        # print(example['frames'].shape)\n",
    "        bs = 30\n",
    "        prev = 2\n",
    "        frame_emb = []\n",
    "        while (prev+bs <= N):\n",
    "            frame_emb += [example['frames'][prev:prev+bs].permute(0,2,3,1)]\n",
    "            prev += bs+2\n",
    "\n",
    "        video_frames = torch.stack(frame_emb).unsqueeze(0).reshape(1, (N-8)//bs, bs, H, W, C)\n",
    "        del frame_emb\n",
    "        \n",
    "        # you can change max_video_len in how2.yaml file as all videos are sanme length\n",
    "        '''BELOW SNIPPET IS COPIED FROM MMFUSION FORWARD. DOES THIS COUNT IN PLAGIARISM?'''\n",
    "        bsz = video_frames.size(0)\n",
    "        assert bsz == 1, \"only bsz=1 is supported now.\"\n",
    "        seq_len = video_frames.size(1)\n",
    "        video_frames = video_frames.view(-1, *video_frames.size()[2:])\n",
    "        vfeats = self.video_encoder(video_frames.permute(0, 4, 1, 2, 3))\n",
    "        vfeats = vfeats['video_embedding']\n",
    "        vfeats = vfeats.view(bsz, seq_len, vfeats.size(-1))\n",
    "        padding = torch.zeros(\n",
    "            bsz, self.max_video_len - seq_len, vfeats.size(-1)).to(device)\n",
    "        \n",
    "        vfeats = torch.cat([vfeats, padding], dim=1)\n",
    "        vmasks = torch.cat([\n",
    "            torch.ones((bsz, seq_len), dtype=torch.bool),\n",
    "            torch.zeros((bsz, self.max_video_len - seq_len), dtype=torch.bool)\n",
    "            ],\n",
    "            dim=1\n",
    "        ).to(device)        \n",
    "        \n",
    "        # faster to batch everything and send, but this works for now\n",
    "        preds = {}\n",
    "        for task, ques_data in example['ques_dict'].items():\n",
    "\n",
    "            caps_all = ques_data['caps']\n",
    "            cmasks_all = ques_data['cmasks']\n",
    "            features = []\n",
    "            for t in range(caps_all.shape[0]):\n",
    "                output = self.mmmodel(caps_all[t].unsqueeze(0), cmasks_all[t].unsqueeze(0), vfeats, vmasks)\n",
    "                features.append(torch.hstack([output['pooled_video'][0], output['pooled_text'][0]]))\n",
    "                                              \n",
    "            # feature vector\n",
    "            # assert features.shape[1] == 768*2\n",
    "            preds[task] = self.head_map[task](torch.stack(features))\n",
    "        \n",
    "        return preds\n",
    "    \n",
    "def dl_collate_fn(data):\n",
    "    return data[0]\n",
    "\n",
    "def ques_to_device(d):\n",
    "    return {k: {k_dash: v_dash.to(device) for k_dash, v_dash in v.items()} for k,v in d.items()}\n",
    "\n",
    "def ans_to_device(d):\n",
    "    return {k: v.to(device) for k,v in d.items()}\n",
    "\n",
    "def process_example(example, transform):\n",
    "    return {\n",
    "        'frames': transform(example['frames'].to(device)),\n",
    "        'ques_dict': ques_to_device(example['ques_dict']),\n",
    "        'ans_dict': ans_to_device(example['ans_dict'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dl, val_dl, optimizer, scheduler=None, max_epochs=10, patience_lim=2, ckpt_freq=1, ckpt_prefix='/scratch/sit/phd/anz228400/col775/COL775A2/vclip_baseline/models/video-clip'):\n",
    "\n",
    "    best_model = None\n",
    "    best_val_loss = 10000\n",
    "    val_losses = []\n",
    "    train_losses = []\n",
    "    val_question_count = {t:0 for t in task_heads}\n",
    "    \n",
    "    patience = 0\n",
    "    \n",
    "    loss_fns = {\n",
    "        'descriptive': nn.CrossEntropyLoss(),\n",
    "        'predictive': nn.BCELoss(),\n",
    "        'explanatory': nn.BCELoss(),\n",
    "        'counterfactual': nn.BCELoss()\n",
    "    }\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "\n",
    "        print(f\"\\n\\n|----------- EPOCH: {epoch} -----------|\")\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        eg_no = 0\n",
    "        for example in tqdm(train_dl):\n",
    "            example = process_example(example, img_transform)\n",
    "                        \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(example)\n",
    "            loss = 0\n",
    "            for task, output in outputs.items():\n",
    "                loss += loss_fns[task](output, example['ans_dict'][task])\n",
    "            \n",
    "            train_loss += loss.detach().cpu()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            eg_no += 1\n",
    "\n",
    "        train_loss /= len(train_dl)\n",
    "        train_losses.append(train_loss)\n",
    " \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for example in tqdm(val_dl):\n",
    "\n",
    "                example = process_example(example, img_transform)\n",
    "\n",
    "                outputs = model(example)\n",
    "                loss = 0\n",
    "                for task, output in outputs.items():\n",
    "                    loss += loss_fns[task](output, example['ans_dict'][task])\n",
    "\n",
    "                val_loss += loss.detach().cpu()\n",
    "\n",
    "        val_loss /= len(val_dl)\n",
    "        val_losses.append(val_loss)\n",
    "            \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        \n",
    "        if (epoch+1)%ckpt_freq == 0:\n",
    "            print('Checkpointing model...')\n",
    "            torch.save(model, f'{ckpt_prefix}-{epoch+1}.pt')\n",
    "          \n",
    "        # if val_loss >= best_val_loss:\n",
    "        #     if patience >= patience_lim:\n",
    "        #         break\n",
    "        #     else:\n",
    "        #         patience += 1\n",
    "        # else:\n",
    "        #     patience = 0\n",
    "        #     best_val_loss = val_loss\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing MMBertForEncoder: ['bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.11.output.dense.weight', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'cls.predictions.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'cls.seq_relationship.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'cls.seq_relationship.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.10.intermediate.dense.bias']\n",
      "- This IS expected if you are initializing MMBertForEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MMBertForEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MMBertForEncoder were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['videomlp.LayerNorm.bias', 'videomlp.linear1.weight', 'videomlp.LayerNorm.weight', 'videomlp.linear2.bias', 'videomlp.linear2.weight', 'videomlp.linear1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/sit/phd/anz228400/scratch/envs_conda/fariseq/lib/python3.8/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "n_epochs=20\n",
    "img_transform = torchvision.transforms.Compose([torchvision.transforms.Resize((224,224))])\n",
    "\n",
    "vclip_model, tokenizer, aligner = MMPTModel.from_pretrained(\"projects/retri/videoclip/how2.yaml\")\n",
    "model = VideoCLIPModel(vclip_model).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs, verbose=False)\n",
    "\n",
    "train_ds = CLEVRERDataset(\"../../../data/data/train\", \"../../../COL775A2_data/frames\", tokenizer, aligner)\n",
    "val_ds = CLEVRERDataset(\"../../../data/data/validation\", \"../../../COL775A2_data/frames\", tokenizer, aligner)\n",
    "val_ds.json_data = val_ds.json_data[:1000]\n",
    "\n",
    "DEBUG = True\n",
    "if DEBUG:\n",
    "    train_ds.json_data = train_ds.json_data[:128]\n",
    "    #train_ds.json_data = [train_ds.json_data[322]]\n",
    "    val_ds.json_data = val_ds.json_data[:32]\n",
    "    n_epochs=2\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=1, collate_fn=dl_collate_fn, shuffle=True, num_workers=8, pin_memory=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=1, collate_fn=dl_collate_fn, shuffle=False, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "|----------- EPOCH: 0 -----------|\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                              | 0/128 [00:00<?, ?it/s]/home/sit/phd/anz228400/scratch/envs_conda/fariseq/lib/python3.8/site-packages/transformers/modeling_utils.py:862: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "100%|████████████████████| 128/128 [04:29<00:00,  2.10s/it]\n",
      "100%|██████████████████████| 32/32 [00:33<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpointing model...\n",
      "\n",
      "\n",
      "|----------- EPOCH: 1 -----------|\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████| 128/128 [04:06<00:00,  1.92s/it]\n",
      "100%|██████████████████████| 32/32 [00:33<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpointing model...\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = train(model, train_dl, val_dl, optimizer, max_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([11, 21]), torch.Size([8]))"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['descriptive'].shape, outputs['explanatory'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([786]), torch.Size([1572]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(786)\n",
    "a.shape, torch.hstack([a, a]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bert.modeling_bert import (BertEmbeddings, ACT2FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer, aligner = MMPTModel.from_pretrained(\n",
    "    \"projects/retri/videoclip/how2.yaml\")\n",
    "\n",
    "model.eval()\n",
    "video_frames = torch.randn(1, 2, 30, 224, 224, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B, T, FPS, H, W, C (VideoCLIP is trained on 30 fps of s3d)\n",
    "# video_frames = torch.randn(1, 2, 30, 224, 224, 3)\n",
    "caps, cmasks = aligner._build_text_seq(\n",
    "    tokenizer(\"Some text here. to see the difference\", add_special_tokens=False)[\"input_ids\"]\n",
    ")\n",
    "\n",
    "caps, cmasks = caps[None, :], cmasks[None, :]  # bsz=1\n",
    "\n",
    "with torch.no_grad():\n",
    "    output1 = model(video_frames, caps, cmasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64]), torch.Size([64]), torch.Size([1, 64]), torch.Size([1, 64]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caps, cmasks = aligner._build_text_seq(\n",
    "    tokenizer(\"Some text here. to see the difference\", add_special_tokens=False)[\"input_ids\"]\n",
    ")\n",
    "caps.shape, cmasks.shape , caps[None, :].shape, cmasks[None, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.2378, -0.1651,  0.2407, -0.3896, -0.2060]),\n",
       " tensor([-0.0448,  0.0667, -0.0249,  0.0255, -0.1126]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2['pooled_video'][0, :5], output2['pooled_text'][0, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.2378, -0.1651,  0.2407, -0.3896, -0.2060]),\n",
       " tensor([-0.0407,  0.0440, -0.1203,  0.1038,  0.1075]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1['pooled_video'][0, :5], output1['pooled_text'][0, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.heys"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
