{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b3241a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torchvision\n",
    "from skimage import io\n",
    "import cv2\n",
    "\n",
    "import copy, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "28e28f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1a25e5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessQuestions:\n",
    "    def __init__(self, ques_list):\n",
    "        self.ques_list = ques_list\n",
    "        \n",
    "    def get_questions(self):\n",
    "        \n",
    "        for i, q in enumerate(self.ques_list):\n",
    "            question_type = q['question_type']\n",
    "            question = q['question']\n",
    "            \n",
    "            if question_type == \"descriptive\":\n",
    "                pass\n",
    "            elif question_type == \"explanatory\":\n",
    "                pass\n",
    "            elif question_type == \"predictive\":\n",
    "                pass\n",
    "            elif question_type == \"counterfactual\":\n",
    "                pass \n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        \n",
    "        return # Tokenized Question answer Pairs\n",
    "        \n",
    "        \n",
    "class CLEVRERDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_dir, frame_dir, img_transform=None):\n",
    "        # TODO load annotations\n",
    "        assert os.path.isdir(data_dir)\n",
    "        assert os.path.isdir(frame_dir)\n",
    "        \n",
    "        with open(os.path.join(data_dir, data_dir.split(\"/\")[-1] + \".json\"), \"r\") as f:\n",
    "            self.json_data = json.load(f)\n",
    "        self.frame_dir = frame_dir\n",
    "        \n",
    "        self.img_transform = img_transform     \n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        # get length from directory\n",
    "        return len(self.json_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "        1. Change here hardcoded path in frame_paths to os.path.join(self.frame_dir, f\"sim_{vid_id}\", \"*.png\")\n",
    "        2. Check normalization mean and std values used in image transform\n",
    "        3. Add tokenized questions + concatinate options (where applicable) and answer token\n",
    "        \"\"\"\n",
    "        \n",
    "        vid_json = self.json_data[idx]\n",
    "        vid_id = vid_json['scene_index']\n",
    "        frame_paths = glob(os.path.join(\"../../../CLEVRER/frames\", \"sim_\" + \"00005\", \"*.png\"))\n",
    "        frames = torch.cat([self.img_transform(io.imread(img)).unsqueeze(0) for img in frame_paths])        \n",
    "        \n",
    "        process_questions = ProcessQuestions(vid_json['questions'])\n",
    "        \n",
    "        return {'frames': frames}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4774bdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "img_transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                                torchvision.transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                                     [0.229, 0.224, 0.225])])\n",
    "train_dataset = CLEVRERDataset(data_dir=\"../../../data/train\", frame_dir=\"../../../CLEVRER/frames\", img_transform=img_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4)\n",
    "for i, data in enumerate(train_loader):\n",
    "    frames = data['frames'].squeeze() #torch.Size([128, 3, 320, 480]) \n",
    "#     print(frames.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bd5f1fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_y, dim_x, max_len=300, p=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        self.pe = torch.zeros(max_len, dim_y, dim_x)\n",
    "\n",
    "        pos = torch.arange(0,max_len).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        div_term_x = torch.exp(torch.arange(0, dim_x, 2).expand((dim_y//2,dim_x//2)) * -(np.log(10000.0) / dim_x))\n",
    "        div_term_y = torch.exp(torch.arange(0, dim_y, 2).unsqueeze(1).expand((dim_y//2,dim_x//2)) * -(np.log(10000.0) / dim_y))\n",
    "\n",
    "        self.pe[:, 0::2, 0::2] = (torch.sin(pos * div_term_x) + torch.sin(pos * div_term_y))/2\n",
    "        self.pe[:, 1::2, 1::2] = (torch.cos(pos * div_term_x) + torch.cos(pos * div_term_y))/2\n",
    "        self.pe = self.pe.unsqueeze(0).repeat(3,1,1,1).transpose(0,1).reshape((3*max_len,dim_y,dim_x))\n",
    "        # assert((self.pe[0] == self.pe[1]) & (self.pe[1] == self.pe[2])).all()\n",
    "        # self.pe = self.pe.unsqueeze(0)\n",
    "        # self.register_buffer(\"pe\", self.pe)\n",
    "\n",
    "        # self.dropout = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        print(self.pe.shape)\n",
    "        return x+self.pe[:x.shape[1],:,:].requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "89756717",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertCNNModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = torchvision.models.resnet50(pretrained=True)\n",
    "        self.bert = transformers.BertModel.from_pretrained('bert-base-cased')\n",
    "        \n",
    "        self.pos_emb = PositionalEmbedding(320, 480)\n",
    "        \n",
    "    def forward(self, frames, tokens):\n",
    "        \n",
    "        bert_output = self.bert(**tokens)\n",
    "        cnn_output = self.cnn(self.pos_emb(frames))\n",
    "        \n",
    "        # feature vector - 1768-dimensional\n",
    "        features = torch.hstack([cnn_output, bert_output.pooler_output])\n",
    "        \n",
    "        return features\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "afc2b326",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DescriptiveTaskHead(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, n_classes=21, p=0.2):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.clf = nn.Sequential(\n",
    "\t\t\tnn.Linear(1768, 1024),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.Linear(1024, 1024),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(1024, n_classes)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, features):\n",
    "\t\t# features: (b,1768)\n",
    "\t\treturn self.clf(features)\n",
    "\n",
    "class ExplanatoryTaskHead(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, p=0.2):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.clf = nn.Sequential(\n",
    "\t\t\tnn.Linear(1768, 1024),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.Linear(1024, 1024),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(1024, 1),\n",
    "            nn.Sigmoid()\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, features):\n",
    "\t\t# features: (b,1768)\n",
    "\t\treturn self.clf(features).squeeze()\n",
    "\n",
    "class PredictiveTaskHead(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, n_classes=2, p=0.2):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.clf = nn.Sequential(\n",
    "\t\t\tnn.Linear(1768, 1024),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.Linear(1024, 1024),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(1024, n_classes)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, features):\n",
    "\t\t# features: (b,1768)\n",
    "\t\treturn self.clf(features)\n",
    "\n",
    "class CounterfactualTaskHead(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, p=0.2):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.clf = nn.Sequential(\n",
    "\t\t\tnn.Linear(1768, 1024),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.Linear(1024, 1024),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(1024, 1),\n",
    "            nn.Sigmoid()\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, features):\n",
    "\t\t# features: (b,1768)\n",
    "\t\treturn self.clf(features).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "51b12163",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = ['0', '1', '2', '3', '4', '5', 'yes', 'no', 'rubber', 'metal', 'sphere', 'cube', 'cylinder', 'gray', 'brown', 'green', 'red', 'blue', 'purple', 'yellow', 'cyan']\n",
    "option_id_map = {\n",
    "    o:i for i,o in enumerate(options)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891143e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9a2ed307",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertCNNModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "38519a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a9f740fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "570919e69874425a99f78d8969500121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2cf0e56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pos_emb.pe = model.pos_emb.pe.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d020f3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 320, 480])\n",
      "torch.Size([900, 320, 480])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8280, -0.1636, -0.5451,  ...,  0.9999, -0.9072,  0.9909],\n",
       "        [-0.9929, -0.5160, -0.4730,  ...,  0.9999, -0.9261,  0.9942]],\n",
       "       device='cuda:0', grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks = tokenizer(['How many collisions happen in this frame?', 'What would happen if the blue ball disappeared?'], return_tensors='pt').to(device)\n",
    "img = torch.randn((2,3,320,480)).to(device)\n",
    "model(img, toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd97d65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../../data/train/train.json\", \"r\") as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "758c9204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "058e10d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                                torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                                                     (0.2023, 0.1994, 0.2010))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2ba89e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_json = json_data[0]\n",
    "vid_id = vid_json['scene_index']\n",
    "frame_paths = glob(os.path.join(\"../../../CLEVRER/frames\", \"sim_\" + \"00005\", \"*.png\"))\n",
    "frames = [img_transform(io.imread(img)) for img in frame_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2ef5f1ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 320, 480])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5d3ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
