{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6163f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torchvision\n",
    "from skimage import io\n",
    "import cv2\n",
    "\n",
    "import copy, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c9b1d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "options = ['0', '1', '2', '3', '4', '5', 'yes', 'no', 'rubber', 'metal', 'sphere', 'cube', 'cylinder', 'gray', 'brown', 'green', 'red', 'blue', 'purple', 'yellow', 'cyan']\n",
    "option_id_map = {\n",
    "    o:i for i,o in enumerate(options)\n",
    "}\n",
    "id_option_map = {\n",
    "    i:o for i,o in enumerate(options)\n",
    "}\n",
    "task_heads = ['descriptive', 'explanatory', 'predictive', 'counterfactual']\n",
    "binary_id_map = {'wrong': 0, 'correct': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "306cbca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessQuestions:\n",
    "    def __init__(self, task_head):\n",
    "        self.task_head = task_head\n",
    "        \n",
    "    def get_qa_batch(self, ques_list):\n",
    "        #TODO: get qa batches for the current task_head\n",
    "        \n",
    "        if self.task_head == \"descriptive\":\n",
    "            return self._get_descriptive_qa(ques_list)\n",
    "        elif self.task_head == \"explanatory\":\n",
    "            return self._get_explanatory_qa(ques_list)\n",
    "        elif self.task_head == \"predictive\":\n",
    "            return self._get_predictive_qa(ques_list)\n",
    "        elif self.task_head == \"counterfactual\":\n",
    "            return self._get_counterfactual_qa(ques_list)\n",
    "        else:\n",
    "            pass       \n",
    "        \n",
    "        return # Tokenized Question answer Pairs\n",
    "    \n",
    "    def _get_descriptive_qa(self, ques_list):\n",
    "        '''\n",
    "        ques_list: list of question_data dictionary\n",
    "        question_list: list of <question> [SEP] <question_subtype>\n",
    "        answer_list: list of respective answer as option_id_map\n",
    "        '''\n",
    "        question_list = list()\n",
    "        answer_list = list()\n",
    "        \n",
    "        for j, q in enumerate(ques_list):\n",
    "            \n",
    "            if q['question_type'] == self.task_head:                \n",
    "                question = q['question']\n",
    "                question_subtype = q['question_subtype']\n",
    "                answer = q['answer']\n",
    "                \n",
    "                question_list.append(question + \" [SEP] \" + question_subtype)\n",
    "                answer_list.append(option_id_map[answer])\n",
    "        \n",
    "        return tokenizer(question_list, return_tensors='pt', padding=True), answer_list\n",
    "    \n",
    "    def _get_explanatory_qa(self, ques_list):\n",
    "        '''\n",
    "        ques_list: list of question_data dictionary\n",
    "        question_list: list of <question> [SEP] <choice_k>\n",
    "        answer_list: list of respective answer as binary_id_map correct = 1 / wrong = 0\n",
    "        '''\n",
    "        question_list = list()\n",
    "        answer_list = list()\n",
    "        \n",
    "        for j, q in enumerate(ques_list):\n",
    "            \n",
    "            if q['question_type'] == self.task_head:                \n",
    "                question = q['question']\n",
    "                \n",
    "                for c, choice in enumerate(q['choices']):\n",
    "                    question_list.append(question + \" [SEP] \" + choice['choice'])\n",
    "                    answer_list.append(binary_id_map[choice['answer']])\n",
    "            \n",
    "        if len(question_list) > 0:\n",
    "            return tokenizer(question_list, return_tensors='pt', padding=True), answer_list\n",
    "        else:\n",
    "            return torch.LongTensor([]), torch.LongTensor([]) # HANDLE THIS IN TRAINING\n",
    "        \n",
    "    def _get_predictive_qa(self, ques_list):\n",
    "        '''\n",
    "        ques_list: list of question_data dictionary\n",
    "        question_list: list of <question> [SEP] <choice_k>\n",
    "        answer_list: list of respective answer as binary_id_map correct = 1 / wrong = 0\n",
    "        '''\n",
    "        question_list = list()\n",
    "        answer_list = list()\n",
    "        \n",
    "        for j, q in enumerate(ques_list):\n",
    "            \n",
    "            if q['question_type'] == self.task_head:                \n",
    "                question = q['question']\n",
    "                \n",
    "                for c, choice in enumerate(q['choices']):\n",
    "                    question_list.append(question + \" [SEP] \" + choice['choice'])\n",
    "                    answer_list.append(binary_id_map[choice['answer']])\n",
    "                    \n",
    "#         print(len(question_list), question_list, len(answer_list), answer_list)\n",
    "        if len(question_list) > 0:\n",
    "            return tokenizer(question_list, return_tensors='pt', padding=True), answer_list\n",
    "        else:\n",
    "            return torch.LongTensor([]), torch.LongTensor([]) # HANDLE THIS IN TRAINING\n",
    "    \n",
    "    def _get_counterfactual_qa(self, ques_list):\n",
    "        '''\n",
    "        ques_list: list of question_data dictionary\n",
    "        question_list: list of <question> [SEP] <choice_k>\n",
    "        answer_list: list of respective answer as binary_id_map correct = 1 / wrong = 0\n",
    "        '''\n",
    "        question_list = list()\n",
    "        answer_list = list()\n",
    "        \n",
    "        for j, q in enumerate(ques_list):\n",
    "            \n",
    "            if q['question_type'] == self.task_head:                \n",
    "                question = q['question']\n",
    "                \n",
    "                for c, choice in enumerate(q['choices']):\n",
    "                    question_list.append(question + \" [SEP] \" + choice['choice'])\n",
    "                    answer_list.append(binary_id_map[choice['answer']])\n",
    "            \n",
    "        if len(question_list) > 0:\n",
    "            return tokenizer(question_list, return_tensors='pt', padding=True), answer_list\n",
    "        else:\n",
    "            return torch.LongTensor([]), torch.LongTensor([]) # HANDLE THIS IN TRAINING\n",
    "        \n",
    "class CLEVRERDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_dir, frame_dir, task_head='descriptive', img_transform=None):\n",
    "        # TODO load annotations\n",
    "        assert os.path.isdir(data_dir)\n",
    "        assert os.path.isdir(frame_dir)\n",
    "        \n",
    "        with open(os.path.join(data_dir, data_dir.split(\"/\")[-1] + \".json\"), \"r\") as f:\n",
    "            self.json_data = json.load(f)\n",
    "        self.frame_dir = frame_dir\n",
    "        self.task_head = task_head\n",
    "        \n",
    "        self.img_transform = img_transform\n",
    "        self.process_questions = ProcessQuestions(task_head)\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        # get length from directory\n",
    "        return len(self.json_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "        1. Change here hardcoded path in frame_paths to os.path.join(self.frame_dir, f\"sim_{vid_id}\", \"*.png\")\n",
    "        2. Check normalization mean and std values used in image transform\n",
    "        3. Add tokenized questions + concatinate options (where applicable) and answer token\n",
    "        4. There are certain videos for which there are no predictive questions. Handle it during training loop\n",
    "            coz dataloader will return torch.LongTensor([]), torch.LongTensor([]). This may happen for explanatory and counterfactual questions as well.\n",
    "        \"\"\"\n",
    "        \n",
    "        vid_json = self.json_data[idx]\n",
    "        vid_id = vid_json['scene_index']\n",
    "        frame_paths = glob(os.path.join(\"../../../CLEVRER/frames\", \"sim_\" + \"00005\", \"*.png\"))\n",
    "        frames = torch.cat([self.img_transform(io.imread(img)).unsqueeze(0) for img in frame_paths])        \n",
    "                \n",
    "        ques_toks, answers = self.process_questions.get_qa_batch(vid_json['questions'])\n",
    "#         answers = torch.LongTensor(answers)\n",
    "        return {'frames': frames, 'ques_toks': ques_toks, 'answers': answers}\n",
    "    \n",
    "def get_task_head(epoch):\n",
    "    task_head = ''\n",
    "    for t in range(4):\n",
    "        if (epoch+1) % (t+1) == 0:\n",
    "            task_head = task_heads[t]\n",
    "    return task_head\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57221c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "|----------- EPOCH: 0 -----------|\n",
      "Training for descriptive task head.\n",
      "\n",
      "SHAPES frames: torch.Size([128, 3, 320, 480]), ques_toks['input_ids']: torch.Size([1, 11, 23]), ques_toks['token_type_ids']: torch.Size([1, 11, 23]), ques_toks['attention_mask']: torch.Size([1, 11, 23]), answers: [tensor([14]), tensor([10]), tensor([10]), tensor([0]), tensor([2]), tensor([6]), tensor([9]), tensor([14]), tensor([3]), tensor([1]), tensor([6])]\n",
      "\n",
      "\n",
      "|----------- EPOCH: 1 -----------|\n",
      "Training for explanatory task head.\n",
      "\n",
      "SHAPES frames: torch.Size([128, 3, 320, 480]), ques_toks['input_ids']: torch.Size([1, 7, 36]), ques_toks['token_type_ids']: torch.Size([1, 7, 36]), ques_toks['attention_mask']: torch.Size([1, 7, 36]), answers: [tensor([0]), tensor([1]), tensor([0]), tensor([1]), tensor([1]), tensor([1]), tensor([1])]\n",
      "\n",
      "\n",
      "|----------- EPOCH: 2 -----------|\n",
      "Training for predictive task head.\n",
      "\n",
      "SHAPES frames: torch.Size([128, 3, 320, 480]), ques_toks['input_ids']: torch.Size([1, 2, 19]), ques_toks['token_type_ids']: torch.Size([1, 2, 19]), ques_toks['attention_mask']: torch.Size([1, 2, 19]), answers: [tensor([0]), tensor([1])]\n",
      "\n",
      "\n",
      "|----------- EPOCH: 3 -----------|\n",
      "Training for counterfactual task head.\n",
      "\n",
      "SHAPES frames: torch.Size([128, 3, 320, 480]), ques_toks['input_ids']: torch.Size([1, 8, 26]), ques_toks['token_type_ids']: torch.Size([1, 8, 26]), ques_toks['attention_mask']: torch.Size([1, 8, 26]), answers: [tensor([0]), tensor([0]), tensor([0]), tensor([1]), tensor([0]), tensor([1]), tensor([0]), tensor([0])]\n"
     ]
    }
   ],
   "source": [
    "# TODO create argsparser\n",
    "epochs = 4\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "img_transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                                torchvision.transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                                     [0.229, 0.224, 0.225])])\n",
    "train_dataset = CLEVRERDataset(data_dir=\"../../../data/train\", frame_dir=\"../../../CLEVRER/frames\", img_transform=img_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\n\\n|----------- EPOCH: {epoch} -----------|\")\n",
    "    task_head = get_task_head(epoch)\n",
    "    train_loader.dataset.task_head = task_head\n",
    "    train_loader.dataset.process_questions = ProcessQuestions(task_head)\n",
    "    print(f\"Training for {train_loader.dataset.task_head} task head.\")\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "        frames = data['frames'].squeeze().to(device) #torch.Size([128, 3, 320, 480])\n",
    "        ques_toks = data['ques_toks'] # returns [N, k_question || choice, toks_len] for bert we may need to squeeze ques_toks['input_ids'],ques_toks['token_type_ids'], ques_toks['attention_mask'] \n",
    "        answers = data['answers']\n",
    "        print(\"\\nSHAPES frames: {}, ques_toks['input_ids']: {}, ques_toks['token_type_ids']: {}, ques_toks['attention_mask']: {}, answers: {}\".format(frames.shape, ques_toks['input_ids'].shape, ques_toks['token_type_ids'].shape, \n",
    "              ques_toks['attention_mask'].shape, answers))\n",
    "        break\n",
    "    #     print(frames.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "cca1fec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_y, dim_x, max_len=300, p=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        self.pe = torch.zeros(max_len, dim_y, dim_x)\n",
    "\n",
    "        pos = torch.arange(0,max_len).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        div_term_x = torch.exp(torch.arange(0, dim_x, 2).expand((dim_y//2,dim_x//2)) * -(np.log(10000.0) / dim_x))\n",
    "        div_term_y = torch.exp(torch.arange(0, dim_y, 2).unsqueeze(1).expand((dim_y//2,dim_x//2)) * -(np.log(10000.0) / dim_y))\n",
    "\n",
    "        self.pe[:, 0::2, 0::2] = (torch.sin(pos * div_term_x) + torch.sin(pos * div_term_y))/2\n",
    "        self.pe[:, 1::2, 1::2] = (torch.cos(pos * div_term_x) + torch.cos(pos * div_term_y))/2\n",
    "        self.pe = self.pe.unsqueeze(0).repeat(3,1,1,1).transpose(0,1).reshape((3*max_len,dim_y,dim_x))\n",
    "        # assert((self.pe[0] == self.pe[1]) & (self.pe[1] == self.pe[2])).all()\n",
    "        # self.pe = self.pe.unsqueeze(0)\n",
    "        # self.register_buffer(\"pe\", self.pe)\n",
    "\n",
    "        # self.dropout = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        print(self.pe.shape)\n",
    "        return x+self.pe[:x.shape[1],:,:].requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ccf9d838",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertCNNModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = torchvision.models.resnet50(pretrained=True)\n",
    "        self.bert = transformers.BertModel.from_pretrained('bert-base-cased')\n",
    "        \n",
    "        self.pos_emb = PositionalEmbedding(320, 480)\n",
    "        \n",
    "    def forward(self, frames, tokens):\n",
    "        \n",
    "        bert_output = self.bert(**tokens)\n",
    "        cnn_output = self.cnn(self.pos_emb(frames))\n",
    "        \n",
    "        # feature vector - 1768-dimensional\n",
    "        features = torch.hstack([cnn_output, bert_output.pooler_output])\n",
    "        \n",
    "        return features\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "75fd5bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DescriptiveTaskHead(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, n_classes=21, p=0.2):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.clf = nn.Sequential(\n",
    "\t\t\tnn.Linear(1768, 1024),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.Linear(1024, 1024),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(1024, n_classes)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, features):\n",
    "\t\t# features: (b,1768)\n",
    "\t\treturn self.clf(features)\n",
    "\n",
    "class ExplanatoryTaskHead(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, p=0.2):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.clf = nn.Sequential(\n",
    "\t\t\tnn.Linear(1768, 1024),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.Linear(1024, 1024),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(1024, 1),\n",
    "            nn.Sigmoid()\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, features):\n",
    "\t\t# features: (b,1768)\n",
    "\t\treturn self.clf(features).squeeze()\n",
    "\n",
    "class PredictiveTaskHead(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, n_classes=2, p=0.2):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.clf = nn.Sequential(\n",
    "\t\t\tnn.Linear(1768, 1024),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.Linear(1024, 1024),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(1024, n_classes)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, features):\n",
    "\t\t# features: (b,1768)\n",
    "\t\treturn self.clf(features)\n",
    "\n",
    "class CounterfactualTaskHead(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, p=0.2):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.clf = nn.Sequential(\n",
    "\t\t\tnn.Linear(1768, 1024),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.Linear(1024, 1024),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(1024, 1),\n",
    "            nn.Sigmoid()\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, features):\n",
    "\t\t# features: (b,1768)\n",
    "\t\treturn self.clf(features).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "169eaeaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5493df97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "33ec7a88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertCNNModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b4a9e98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c38bc561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "570919e69874425a99f78d8969500121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "252fa6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pos_emb.pe = model.pos_emb.pe.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4b0c0dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 320, 480])\n",
      "torch.Size([900, 320, 480])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8280, -0.1636, -0.5451,  ...,  0.9999, -0.9072,  0.9909],\n",
       "        [-0.9929, -0.5160, -0.4730,  ...,  0.9999, -0.9261,  0.9942]],\n",
       "       device='cuda:0', grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks = tokenizer(['How many collisions happen in this frame?', 'What would happen if the blue ball disappeared?'], return_tensors='pt').to(device)\n",
    "img = torch.randn((2,3,320,480)).to(device)\n",
    "model(img, toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69678ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "08913d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = tokenizer(['How many collisions happen in this frame?', 'What would happen if the blue ball disappeared?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5b7dea05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 11])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.LongTensor(toks['input_ids']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b419c0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                                torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                                                     (0.2023, 0.1994, 0.2010))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e477d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../../data/train/train.json\", \"r\") as f:\n",
    "    json_data = json.load(f)\n",
    "vid_json = json_data[0]\n",
    "vid_id = vid_json['scene_index']\n",
    "frame_paths = glob(os.path.join(\"../../../CLEVRER/frames\", \"sim_\" + \"00005\", \"*.png\"))\n",
    "frames = [img_transform(io.imread(img)) for img in frame_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4439d697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'choice_id': 0,\n",
       "  'choice': 'the collision between the gray sphere and the purple sphere',\n",
       "  'program': ['events',\n",
       "   'objects',\n",
       "   'gray',\n",
       "   'filter_color',\n",
       "   'sphere',\n",
       "   'filter_shape',\n",
       "   'unique',\n",
       "   'filter_collision',\n",
       "   'objects',\n",
       "   'purple',\n",
       "   'filter_color',\n",
       "   'sphere',\n",
       "   'filter_shape',\n",
       "   'unique',\n",
       "   'filter_collision',\n",
       "   'unique'],\n",
       "  'answer': 'wrong'},\n",
       " {'choice_id': 1,\n",
       "  'choice': 'the presence of the metal sphere',\n",
       "  'program': ['objects',\n",
       "   'metal',\n",
       "   'filter_material',\n",
       "   'sphere',\n",
       "   'filter_shape',\n",
       "   'unique'],\n",
       "  'answer': 'wrong'},\n",
       " {'choice_id': 2,\n",
       "  'choice': \"the blue rubber sphere's entering the scene\",\n",
       "  'program': ['events',\n",
       "   'objects',\n",
       "   'blue',\n",
       "   'filter_color',\n",
       "   'rubber',\n",
       "   'filter_material',\n",
       "   'sphere',\n",
       "   'filter_shape',\n",
       "   'unique',\n",
       "   'filter_in',\n",
       "   'unique'],\n",
       "  'answer': 'correct'},\n",
       " {'choice_id': 3,\n",
       "  'choice': 'the presence of the blue rubber sphere',\n",
       "  'program': ['objects',\n",
       "   'blue',\n",
       "   'filter_color',\n",
       "   'rubber',\n",
       "   'filter_material',\n",
       "   'sphere',\n",
       "   'filter_shape',\n",
       "   'unique'],\n",
       "  'answer': 'correct'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vid_json['questions'][11]['choices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd3552b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
