{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torchvision\n",
    "# from skimage import io\n",
    "import cv2\n",
    "\n",
    "import copy, json, pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "sns.set_style('whitegrid')\n",
    "from mmpt.models import MMPTModel\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mmpt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fa5e7439f06a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmmpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMMPTModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mmpt'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "options = ['0', '1', '2', '3', '4', '5', 'yes', 'no', 'rubber', 'metal', 'sphere', 'cube', 'cylinder', 'gray', 'brown', 'green', 'red', 'blue', 'purple', 'yellow', 'cyan']\n",
    "option_id_map = {\n",
    "    o:i for i,o in enumerate(options)\n",
    "}\n",
    "id_option_map = {\n",
    "    i:o for i,o in enumerate(options)\n",
    "}\n",
    "task_heads = ['descriptive', 'explanatory', 'predictive', 'counterfactual']\n",
    "binary_id_map = {'wrong': 0, 'correct': 1}\n",
    "id_binary_map = {0: 'wrong', 1: 'correct'}\n",
    "ques_dict_keys = [\"tokens\", \"q_ids\", \"choice_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/sit/phd/anz228400/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "import json\n",
    "import urllib\n",
    "\n",
    "# Choose the `slowfast_r50` model \n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)\n",
    "model.blocks[6].output_pool = nn.Identity()\n",
    "model = model.to(device)\n",
    "\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    "    UniformCropVideo\n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_url = \"https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json\"\n",
    "json_filename = \"kinetics_classnames.json\"\n",
    "try: urllib.URLopener().retrieve(json_url, json_filename)\n",
    "except: urllib.request.urlretrieve(json_url, json_filename)\n",
    "\n",
    "with open(json_filename, \"r\") as f:\n",
    "    kinetics_classnames = json.load(f)\n",
    "\n",
    "# Create an id to label name mapping\n",
    "kinetics_id_to_classname = {}\n",
    "for k, v in kinetics_classnames.items():\n",
    "    kinetics_id_to_classname[v] = str(k).replace('\"', \"\")\n",
    "    \n",
    "side_size = 256\n",
    "mean = [0.45, 0.45, 0.45]\n",
    "std = [0.225, 0.225, 0.225]\n",
    "crop_size = 256\n",
    "num_frames = 32\n",
    "sampling_rate = 2\n",
    "frames_per_second = 30\n",
    "slowfast_alpha = 4\n",
    "num_clips = 10\n",
    "num_crops = 3\n",
    "\n",
    "class PackPathway(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Transform for converting video frames as a list of tensors. \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, frames: torch.Tensor):\n",
    "        fast_pathway = frames\n",
    "        # Perform temporal sampling from the fast pathway.\n",
    "        slow_pathway = torch.index_select(\n",
    "            frames,\n",
    "            1,\n",
    "            torch.linspace(\n",
    "                0, frames.shape[1] - 1, frames.shape[1] // slowfast_alpha\n",
    "            ).long(),\n",
    "        )\n",
    "        frame_list = [slow_pathway, fast_pathway]\n",
    "        return frame_list\n",
    "\n",
    "def load_input(video_path):\n",
    "    # Select the duration of the clip to load by specifying the start and end duration\n",
    "    # The start_sec should correspond to where the action occurs in the video\n",
    "    start_sec = 0\n",
    "    end_sec = start_sec + clip_duration\n",
    "    # Initialize an EncodedVideo helper class and load the video\n",
    "    video = EncodedVideo.from_path(video_path)\n",
    "\n",
    "    # Load the desired clip\n",
    "    video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
    "    \n",
    "    # video_data['video'] = [i.to(device)[None, ...] for i in video_data['video']]\n",
    "    # Apply a transform to normalize the video input\n",
    "    video_data = transform(video_data)\n",
    "\n",
    "    # Move the inputs to the desired device\n",
    "    inputs = video_data[\"video\"]\n",
    "    # inputs = [i.to(device)[None, ...] for i in inputs]\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "transform =  ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=Compose(\n",
    "        [\n",
    "            UniformTemporalSubsample(num_frames),\n",
    "            Lambda(lambda x: x/255.0),\n",
    "            NormalizeVideo(mean, std),\n",
    "            ShortSideScale(\n",
    "                size=side_size\n",
    "            ),\n",
    "            CenterCropVideo(crop_size),\n",
    "            PackPathway()\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# The duration of the input clip is also specific to the model.\n",
    "clip_duration = (num_frames * sampling_rate)/frames_per_second\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                    | 0/20000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30012/3866906542.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvideo_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mvid_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"video\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sim\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".mp4\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_30012/443338899.py\u001b[0m in \u001b[0;36mload_input\u001b[0;34m(video_path)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mvideo_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvideo_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# Apply a transform to normalize the video input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mvideo_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# Move the inputs to the desired device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \"\"\"\n\u001b[1;32m     74\u001b[0m         return pytorchvideo.transforms.functional.uniform_temporal_subsample(\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_temporal_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         )\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/transforms/functional.py\u001b[0m in \u001b[0;36muniform_temporal_subsample\u001b[0;34m(x, num_samples, temporal_dim)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mAn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0msubsampled\u001b[0m \u001b[0mtemporal\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \"\"\"\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtemporal_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mnum_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# Sample by nearest neighbor interpolation if num_samples > t.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "video_paths = glob(\"/home/sit/phd/anz228400/scratch/col775/data/data/**/video/**/*.mp4\")\n",
    "video_encodings = {}\n",
    "for path in tqdm(video_paths):\n",
    "    inputs = load_input(path)\n",
    "    vid_encoding = model(inputs) \n",
    "    key = path.split(\"/\")[-1].replace(\"video\", \"sim\").replace(\".mp4\",\"\")\n",
    "    video_encodings[key] = vid_encoding.detach().cpu().reshape(-1)\n",
    "\n",
    "with open(\"./video_embeddings_slowfast.pkl\", \"wb\") as file:\n",
    "    pickle.dump(video_encodings, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./video_embeddings_slowfast.pkl\", \"rb\") as file:\n",
    "    loaded = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sim_04405': tensor([-0.2312, -0.7562, -1.4251,  ..., -0.5723, -1.2168, -1.0034]),\n",
       " 'sim_04621': tensor([-1.0843, -1.9800, -0.7465,  ..., -1.5898, -0.9538, -0.9864]),\n",
       " 'sim_04955': tensor([-0.6255, -1.7265, -0.4216,  ..., -1.3604, -0.7075, -0.8999]),\n",
       " 'sim_04874': tensor([ 0.1825,  2.2434, -0.4476,  ..., -1.5830, -1.3686, -0.6018]),\n",
       " 'sim_04096': tensor([ 0.0842, -0.7759,  0.0747,  ..., -3.3722, -0.8777, -2.1704]),\n",
       " 'sim_04679': tensor([ 0.5921, -0.8086,  0.3253,  ..., -0.5025, -0.1994, -1.0696]),\n",
       " 'sim_04628': tensor([-0.4395, -0.1780,  1.2632,  ..., -1.0057, -0.8850, -0.2879]),\n",
       " 'sim_04796': tensor([-1.1042, -0.1151, -0.9841,  ..., -0.7253, -1.6775, -0.6598]),\n",
       " 'sim_04808': tensor([-0.2059, -0.2200, -0.7373,  ...,  0.0241,  0.5525,  1.0700]),\n",
       " 'sim_04470': tensor([-0.0502, -0.1298, -1.4948,  ..., -0.1903, -0.7532, -0.4350]),\n",
       " 'sim_04507': tensor([-0.0208, -0.6247,  0.3692,  ...,  0.6043, -1.1731, -1.4742]),\n",
       " 'sim_04522': tensor([ 0.7611, -0.0977,  0.1413,  ...,  0.3884, -1.0667,  0.0402]),\n",
       " 'sim_04198': tensor([ 0.0990, -0.3199,  0.1491,  ...,  0.8699, -0.0689,  0.2751]),\n",
       " 'sim_04877': tensor([-1.7197, -1.9239, -2.0700,  ..., -0.0043, -1.1535, -0.0320]),\n",
       " 'sim_04784': tensor([-0.2696,  0.1972, -0.6962,  ..., -1.9084, -0.4552, -1.4970]),\n",
       " 'sim_04281': tensor([-0.1775, -1.1175,  0.6236,  ..., -0.0151, -1.0829, -0.9412]),\n",
       " 'sim_04110': tensor([-0.0644,  0.1505, -1.3749,  ..., -0.2686, -2.1208, -0.5082]),\n",
       " 'sim_04783': tensor([-1.5056e+00, -3.2957e-01, -6.1306e-01,  ..., -5.3877e-04,\n",
       "         -4.4830e-01,  2.9218e-01]),\n",
       " 'sim_04375': tensor([-0.4805,  0.9874, -0.3052,  ...,  0.5999, -0.1005, -0.3089]),\n",
       " 'sim_04456': tensor([ 0.1620, -0.5244, -1.1339,  ..., -2.1538, -0.3785,  0.2173]),\n",
       " 'sim_04511': tensor([-1.1315, -1.0149, -0.6325,  ...,  0.1805,  0.8123,  0.7735]),\n",
       " 'sim_04592': tensor([-1.5184, -1.6777, -0.2800,  ...,  0.4889, -1.6049,  0.1067]),\n",
       " 'sim_04954': tensor([-0.4471, -1.5738, -0.0702,  ..., -1.3820,  0.0811, -0.6975]),\n",
       " 'sim_04918': tensor([ 9.1006e-01,  1.7285e-03,  1.8163e+00,  ..., -4.8651e-02,\n",
       "         -5.0344e-01, -1.0911e+00]),\n",
       " 'sim_04886': tensor([-0.4282,  0.1837,  0.0547,  ..., -0.7355,  0.8548,  0.0334]),\n",
       " 'sim_04880': tensor([-0.4733,  0.3017,  0.5806,  ..., -0.1570,  0.1233, -0.1455]),\n",
       " 'sim_04927': tensor([-0.6179, -0.2324, -0.6090,  ...,  0.5517, -0.4799, -0.2766]),\n",
       " 'sim_04643': tensor([ 0.9144,  0.7971,  0.0823,  ..., -1.9095, -1.5030, -1.4001]),\n",
       " 'sim_04640': tensor([ 0.1751, -0.6080, -1.3159,  ..., -0.7945, -1.3167,  0.7250]),\n",
       " 'sim_04442': tensor([-0.8574, -0.9533, -1.5464,  ..., -0.9099, -0.2227, -1.6471]),\n",
       " 'sim_04468': tensor([-0.6861,  0.2345, -1.5392,  ..., -1.4340,  0.2543, -0.8747]),\n",
       " 'sim_04366': tensor([-0.0700,  0.0357, -0.5837,  ..., -0.1995, -0.9117,  0.5003]),\n",
       " 'sim_04754': tensor([-1.6340, -1.2801, -0.0478,  ..., -2.0680, -2.1128,  0.0267]),\n",
       " 'sim_04729': tensor([-0.9992, -0.0953,  0.2474,  ..., -0.3319,  0.2043, -0.7445]),\n",
       " 'sim_04378': tensor([-1.6546, -0.6927, -1.3778,  ...,  0.6393,  0.1432, -0.0299]),\n",
       " 'sim_04834': tensor([-1.3753, -0.8042, -0.3153,  ..., -0.3308,  1.1879, -0.1762]),\n",
       " 'sim_04066': tensor([-0.0010, -0.0025,  0.2312,  ...,  0.0704,  0.2518, -0.3779]),\n",
       " 'sim_04331': tensor([-0.5613, -1.2424, -0.6958,  ..., -0.7006, -0.6131, -0.6685]),\n",
       " 'sim_04953': tensor([-0.3222, -0.0612, -0.7201,  ..., -0.3849, -0.0738,  0.0872]),\n",
       " 'sim_04506': tensor([-0.4632, -1.0220,  0.8049,  ...,  0.1327, -0.6233, -0.6526]),\n",
       " 'sim_04039': tensor([-0.0758, -0.4512, -0.4428,  ..., -1.3955, -0.4386, -0.3538]),\n",
       " 'sim_04846': tensor([ 0.1257, -0.8520, -0.3996,  ...,  1.0853,  2.4114,  0.2695]),\n",
       " 'sim_04761': tensor([-1.5336, -1.2228, -1.5977,  ..., -1.8040, -0.4110, -0.6287]),\n",
       " 'sim_04396': tensor([-0.0899, -1.0774, -0.9407,  ..., -0.6406,  0.6804,  0.7515]),\n",
       " 'sim_04821': tensor([-0.8640, -0.6237, -1.1723,  ..., -1.5925, -1.5606, -1.6619]),\n",
       " 'sim_04287': tensor([-0.3141,  0.7282, -0.5720,  ..., -0.2054, -1.1069,  0.4304]),\n",
       " 'sim_04820': tensor([-1.6048, -1.2194, -1.3181,  ...,  1.1257, -0.6907, -0.1431]),\n",
       " 'sim_04777': tensor([-1.1114, -1.3196, -0.7428,  ..., -0.1250, -1.4247, -1.4228]),\n",
       " 'sim_04922': tensor([-0.8920, -0.7285,  0.5435,  ..., -1.2816, -1.2154, -1.1569]),\n",
       " 'sim_04541': tensor([-0.5942, -0.6223, -0.1001,  ..., -0.1522, -0.4775, -0.8206]),\n",
       " 'sim_04822': tensor([-0.6406, -0.1777, -1.8546,  ...,  0.7271,  1.1895,  1.0202]),\n",
       " 'sim_04184': tensor([-0.7195, -0.6499,  1.3405,  ...,  0.1349,  1.0066, -1.2133]),\n",
       " 'sim_04489': tensor([-1.2315, -0.5896,  0.1482,  ..., -0.6872,  0.2015,  0.4365]),\n",
       " 'sim_04671': tensor([-0.7269,  0.4206, -0.6192,  ..., -0.6299, -1.5154, -0.6777]),\n",
       " 'sim_04451': tensor([-0.7820,  0.6804,  0.6341,  ...,  0.0495, -0.2408, -0.2319]),\n",
       " 'sim_04843': tensor([-0.8232, -0.8355, -0.8765,  ..., -1.1966, -1.2840, -0.8026]),\n",
       " 'sim_04845': tensor([-0.4298, -0.2325, -1.5884,  ..., -1.8033, -0.8532, -1.0699]),\n",
       " 'sim_04271': tensor([-2.7956, -1.9309, -2.3502,  ..., -0.6302, -0.2742, -0.4245]),\n",
       " 'sim_04995': tensor([-0.6118, -0.8978, -1.0088,  ..., -0.2480,  0.2607, -1.2730]),\n",
       " 'sim_04261': tensor([-2.0864, -1.0390, -0.5138,  ..., -1.7203, -0.6962, -1.2894]),\n",
       " 'sim_04073': tensor([ 0.6452, -1.4579, -0.5277,  ...,  0.1487,  0.2421,  0.3219]),\n",
       " 'sim_04292': tensor([-0.2534, -0.4724, -0.3496,  ..., -0.9638, -0.5108, -1.2526]),\n",
       " 'sim_04946': tensor([-1.3840,  0.8535,  0.4113,  ..., -1.7130, -1.0000, -0.7280]),\n",
       " 'sim_04369': tensor([-1.2556e+00, -6.8502e-01, -3.1776e-01,  ...,  6.1332e-04,\n",
       "          1.8848e-01, -1.1280e-01])}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessQuestions:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        pass\n",
    "        \n",
    "    def get_qa_batch(self, ques_list):\n",
    "        '''\n",
    "        INPUT:\n",
    "        ques_list: list of question_data dictionary\n",
    "        OUTPUT: question_dict, answer_dict\n",
    "        descriptive: \n",
    "            question_list: list of <question> [SEP] <question_subtype>\n",
    "            answer_list: list of respective answer as option_id_map\n",
    "        explanatory:\n",
    "            question_list: list of <question> [SEP] <choice_k>\n",
    "            answer_list: list of respective answer as binary_id_map correct = 1 / wrong = 0\n",
    "        predictive:\n",
    "            question_list: list of <question> [SEP] <choice_k>\n",
    "            answer_list: list of respective answer as binary_id_map correct = 1 / wrong = 0\n",
    "        counterfactual:\n",
    "            question_list: list of <question> [SEP] <choice_k>\n",
    "            answer_list: list of respective answer as binary_id_map correct = 1 / wrong = 0\n",
    "        '''\n",
    "        \n",
    "        question_dict = {i:{j:[] for j in ques_dict_keys} for i in task_heads}\n",
    "        answer_dict = {i:[] for i in task_heads}\n",
    "        for j, q in enumerate(ques_list):\n",
    "            question_type = q['question_type']\n",
    "            \n",
    "            if question_type == \"descriptive\":\n",
    "                question_dict[question_type]['tokens'].append(q['question'] + \" [SEP] \" + q['question_subtype'])\n",
    "                question_dict[question_type]['q_ids'].append(q['question_id'])\n",
    "                answer_dict[question_type].append(option_id_map[q['answer']])\n",
    "\n",
    "            elif question_type == \"explanatory\":                \n",
    "                question = q['question']\n",
    "                q_id = q['question_id']                \n",
    "                for c, choice in enumerate(q['choices']):\n",
    "                    question_dict[question_type]['tokens'].append(question + \" [SEP] \" + choice['choice'])\n",
    "                    question_dict[question_type]['q_ids'].append(q_id)\n",
    "                    question_dict[question_type]['choice_ids'].append(choice['choice_id'])\n",
    "                    answer_dict[question_type].append(binary_id_map[choice['answer']])\n",
    "\n",
    "            elif question_type == \"predictive\":               \n",
    "                question = q['question']\n",
    "                q_id = q['question_id']                \n",
    "                for c, choice in enumerate(q['choices']):\n",
    "                    question_dict[question_type]['tokens'].append(question + \" [SEP] \" + choice['choice'])\n",
    "                    question_dict[question_type]['q_ids'].append(q_id)\n",
    "                    question_dict[question_type]['choice_ids'].append(choice['choice_id'])\n",
    "                    answer_dict[question_type].append(binary_id_map[choice['answer']])\n",
    "      \n",
    "            elif question_type == \"counterfactual\":               \n",
    "                question = q['question']\n",
    "                q_id = q['question_id']                \n",
    "                for c, choice in enumerate(q['choices']):\n",
    "                    question_dict[question_type]['tokens'].append(question + \" [SEP] \" + choice['choice'])\n",
    "                    question_dict[question_type]['q_ids'].append(q_id)\n",
    "                    question_dict[question_type]['choice_ids'].append(choice['choice_id'])\n",
    "                    answer_dict[question_type].append(binary_id_map[choice['answer']])\n",
    "        \n",
    "        for th in task_heads:\n",
    "            if answer_dict[th]:\n",
    "                question_dict[th]['tokens'] = self.tokenizer(question_dict[th]['tokens'], return_tensors='pt', padding=True)\n",
    "                question_dict[th]['q_ids'] = torch.tensor(question_dict[th]['q_ids'], dtype=torch.long)\n",
    "                question_dict[th]['choice_ids'] = torch.tensor(question_dict[th]['choice_ids'], dtype=torch.long)\n",
    "                answer_dict[th] = torch.tensor(answer_dict[th], dtype=torch.long)\n",
    "                \n",
    "                if th != 'descriptive':\n",
    "                    answer_dict[th] = answer_dict[th].float()\n",
    "            else:\n",
    "                del question_dict[th]\n",
    "                del answer_dict[th]\n",
    "        \n",
    "        return question_dict, answer_dict\n",
    "        \n",
    "    \n",
    "class CLEVRERDataset(Dataset):\n",
    "    def __init__(self, data_dir, frame_dir, tokenizer):\n",
    "        # TODO load annotations\n",
    "        assert os.path.isdir(data_dir)\n",
    "        assert os.path.isdir(frame_dir)\n",
    "        \n",
    "        with open(os.path.join(data_dir, data_dir.split(\"/\")[-1] + \".json\"), \"r\") as f:\n",
    "            self.json_data = json.load(f)\n",
    "        self.frame_dir = frame_dir\n",
    "        \n",
    "        self.process_questions = ProcessQuestions(tokenizer)\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        # get length from directory\n",
    "        return len(self.json_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "        1. Change here hardcoded path in frame_paths to os.path.join(self.frame_dir, f\"sim_{vid_id}\", \"*.png\")\n",
    "        2. Check normalization mean and std values used in image transform.\n",
    "        \"\"\"\n",
    "        \n",
    "        vid_json = self.json_data[idx]\n",
    "        vid_id = vid_json['scene_index']\n",
    "        \n",
    "        frame_dir = os.path.join(self.frame_dir, f\"sim_{vid_id:05d}\", \"*.png\")\n",
    "        frame_paths = sorted(glob(frame_dir))\n",
    "        frames = torch.stack([torchvision.io.read_image(img).float() for img in frame_paths[::5]])\n",
    "                \n",
    "        ques_dict, ans_dict = self.process_questions.get_qa_batch(vid_json['questions'])\n",
    "\n",
    "        return {'scene_index': vid_id, 'frames': frames, 'ques_dict': ques_dict, 'ans_dict': ans_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DescriptiveTaskHead(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, n_classes=21, p=0.2, input_dim=768*2):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.clf = nn.Sequential(\n",
    "\t\t\tnn.Linear(input_dim, 1024),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.Linear(1024, n_classes)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, features):\n",
    "\t\treturn self.clf(features)\n",
    "\n",
    "class ExplanatoryTaskHead(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, p=0.2, input_dim=768*2):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.clf = nn.Sequential(\n",
    "\t\t\tnn.Linear(input_dim, 1024),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.Linear(1024, 1),\n",
    "            nn.Sigmoid()\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, features):\n",
    "\t\treturn self.clf(features).reshape(-1)\n",
    "\n",
    "class PredictiveTaskHead(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, p=0.2, input_dim=768*2):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.clf = nn.Sequential(\n",
    "\t\t\tnn.Linear(input_dim, 1024),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.Linear(1024, 1),\n",
    "            nn.Sigmoid()\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, features):\n",
    "\t\treturn self.clf(features).reshape(-1)\n",
    "\n",
    "class CounterfactualTaskHead(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, p=0.2, input_dim=768*2):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.clf = nn.Sequential(\n",
    "\t\t\tnn.Linear(input_dim, 1024),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.Linear(1024, 1),\n",
    "            nn.Sigmoid()\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, features):\n",
    "\t\treturn self.clf(features).reshape(-1)\n",
    "\n",
    "class BertCNNModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size=768):\n",
    "        super().__init__()\n",
    "        self.cnn = torchvision.models.resnet50(pretrained=True)\n",
    "        self.cnn.fc = nn.Identity()\n",
    "        \n",
    "        for name, param in self.cnn.named_parameters():\n",
    "            if not name.startswith('layer4'):\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "                input_size=2048,\n",
    "                batch_first=True,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=1\n",
    "            )\n",
    "        self.bert = transformers.BertModel.from_pretrained('bert-base-cased')\n",
    "        \n",
    "        self.h0 = nn.Parameter(torch.empty(1,hidden_size).normal_(0, 0.1))\n",
    "        self.c0 = nn.Parameter(torch.empty(1,hidden_size).normal_(0, 0.1))\n",
    "        \n",
    "        self.descriptive_head = DescriptiveTaskHead()\n",
    "        self.explanatory_head = ExplanatoryTaskHead()\n",
    "        self.predictive_head = PredictiveTaskHead()\n",
    "        self.counterfactual_head = CounterfactualTaskHead()\n",
    "        \n",
    "        self.head_map = {\n",
    "            'descriptive': self.descriptive_head,\n",
    "            'predictive': self.predictive_head,\n",
    "            'explanatory': self.explanatory_head,\n",
    "            'counterfactual': self.counterfactual_head\n",
    "        }\n",
    "        \n",
    "    def forward(self, example):\n",
    "        \n",
    "        N, C, H, W = example['frames'].shape\n",
    "        # i = 0\n",
    "        # bs = 8\n",
    "        # frame_emb = []\n",
    "        # while (i*bs < N):\n",
    "        #     frame_emb += [self.cnn(example['frames'][i*bs:(i+1)*bs])]\n",
    "        #     i += 1\n",
    "        #     \n",
    "        # frame_emb = torch.vstack(frame_emb)\n",
    "        frame_emb = self.cnn(example['frames'])\n",
    "        frame_encs, (video_enc, last_cell_state) = self.lstm(frame_emb, (self.h0, self.c0))\n",
    "        \n",
    "        # faster to batch everything and send, but this works for now\n",
    "        preds = {}\n",
    "        for task, ques_data in example['ques_dict'].items():\n",
    "\n",
    "            bert_output = self.bert(**ques_data['tokens'])\n",
    "\n",
    "            # feature vector\n",
    "            features = torch.hstack([video_enc.repeat(bert_output.pooler_output.size(0),1), bert_output.pooler_output])\n",
    "\n",
    "            preds[task] = self.head_map[task](features)\n",
    "        \n",
    "        return preds\n",
    "\n",
    "\n",
    "def dl_collate_fn(data):\n",
    "    return data[0]\n",
    "\n",
    "def ques_to_device(d):\n",
    "    return {k: {k_dash: v_dash.to(device) for k_dash, v_dash in v.items()} for k,v in d.items()}\n",
    "\n",
    "def ans_to_device(d):\n",
    "    return {k: v.to(device) for k,v in d.items()}\n",
    "\n",
    "def process_example(example, transform):\n",
    "    return {\n",
    "        'frames': transform(example['frames'].to(device)),\n",
    "        'ques_dict': ques_to_device(example['ques_dict']),\n",
    "        'ans_dict': ans_to_device(example['ans_dict'])\n",
    "    }\n",
    "\n",
    "def load_checkpoint(file_path):\n",
    "\n",
    "    print(\"loading model:\", file_path)\n",
    "    model = torch.load(file_path).to(device)\n",
    "    print(\"model load successful...\")\n",
    "\n",
    "    return model\n",
    "\n",
    "class MULTICALSS_PREDS:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def get_pred(self, output):\n",
    "        return output.argmax(dim=1)\n",
    "\n",
    "class BINARY_PREDS:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def get_pred(self, output):\n",
    "        return output.round()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(model, eval_dl, output_prefix=None):\n",
    "    \n",
    "    pred_fns = {\n",
    "        'descriptive': MULTICALSS_PREDS(),\n",
    "        'predictive': BINARY_PREDS(),\n",
    "        'explanatory': BINARY_PREDS(),\n",
    "        'counterfactual': BINARY_PREDS()\n",
    "    }\n",
    "    \n",
    "    pred_dict = {k: [] for k in task_heads}\n",
    "    gold_dict = {k: [] for k in task_heads}\n",
    "    predictions_json = []\n",
    "    accuracy_dict = {k: -1 for k in task_heads}\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for example in tqdm(eval_dl):\n",
    "            scene_pred = {'scene_index': example['scene_index'], 'questions': []}\n",
    "            \n",
    "            example = process_example(example, img_transform)\n",
    "\n",
    "            outputs = model(example)\n",
    "            \n",
    "            for task, output in outputs.items():\n",
    "                pred = pred_fns[task].get_pred(output).detach().to('cpu').tolist()\n",
    "                gold = example['ans_dict'][task].detach().to('cpu').tolist()\n",
    "                pred_dict[task].extend(pred)\n",
    "                gold_dict[task].extend(gold)\n",
    "                \n",
    "                \n",
    "                if task == 'descriptive':\n",
    "                    q_ids = example['ques_dict'][task]['q_ids'].detach().to('cpu').tolist()\n",
    "                    for i in range(len(q_ids)):\n",
    "                        scene_pred['questions'].append({\"question_id\" : q_ids[i], \"answer\": id_option_map[pred[i]]})\n",
    "                \n",
    "                else:\n",
    "                    q_ids = example['ques_dict'][task]['q_ids']\n",
    "                    assert len(q_ids) == len(pred)\n",
    "\n",
    "                    choice_ids = example['ques_dict'][task]['choice_ids'].detach().to('cpu').tolist()\n",
    "                    unique_q_ids = q_ids.unique().detach().to('cpu').tolist()\n",
    "                    temp_ques_list = [{\"question_id\": q, \"choices\": []} for q in unique_q_ids]\n",
    "\n",
    "                    for i, q in enumerate(q_ids):\n",
    "                        temp_ques_list[unique_q_ids.index(q)][\"choices\"].append({\"choice_id\": choice_ids[i], \"answer\": id_binary_map[pred[i]]})\n",
    "\n",
    "                    scene_pred['questions'].extend(temp_ques_list)\n",
    "\n",
    "            predictions_json.append(scene_pred)            \n",
    "                \n",
    "                    \n",
    "                     \n",
    "                \n",
    "    for th in task_heads:\n",
    "        accuracy_dict[th] = accuracy_score(gold_dict[th], pred_dict[th])\n",
    "    \n",
    "    with open(f\"{output_prefix}-predictions.json\", \"w+\") as file:\n",
    "        json.dump(predictions_json, file)\n",
    "\n",
    "    with open(f\"{output_prefix}-accuracy.json\", \"w+\") as file:\n",
    "        json.dump(accuracy_dict, file)\n",
    "         \n",
    "    with open(f\"{output_prefix}-gold.json\", \"w+\") as file:\n",
    "        json.dump(gold_dict, file)\n",
    "    \n",
    "    with open(f\"{output_prefix}-pred.json\", \"w+\") as file:\n",
    "        json.dump(pred_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model: ../models/baseline-5.pt\n",
      "model load successful...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████| 8/8 [00:02<00:00,  3.29it/s]\n"
     ]
    }
   ],
   "source": [
    "img_transform = torchvision.transforms.Compose([torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                                                 (0.2023, 0.1994, 0.2010))])\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "eval_ds = CLEVRERDataset(\"../../../data/data/validation\", \"../../../clevrer_code/frames\", tokenizer)\n",
    "\n",
    "DEBUG = True\n",
    "if DEBUG:\n",
    "    eval_ds.json_data = eval_ds.json_data[:8]\n",
    "\n",
    "eval_dl = DataLoader(eval_ds, batch_size=1, collate_fn=dl_collate_fn, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "model_file = '../models/baseline-5.pt'\n",
    "\n",
    "model = load_checkpoint(model_file)\n",
    "\n",
    "model_eval(model, eval_dl, f'../results/{model_file.split(\"/\")[-1].split(\".\")[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'baseline-5'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_file = '../models/baseline-5.pt'\n",
    "model_file.split(\"/\")[-1].split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 0., 0., 0., 1., 1., 0., 1.], device='cuda:0'),\n",
       " tensor([0., 0., 1., 1., 1., 1., 0., 1.], device='cuda:0'))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th = 0.5\n",
    "\n",
    "outputs['explanatory'].round(), example['ans_dict']['explanatory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'descriptive': tensor([10, 17,  8,  3,  2,  7, 19, 11,  1,  1,  7], device='cuda:0'),\n",
       "  'explanatory': tensor([0., 0., 1., 1., 1., 1., 0., 1.], device='cuda:0'),\n",
       "  'counterfactual': tensor([0., 0., 0., 1., 0., 1., 1., 0.], device='cuda:0')},\n",
       " dict_keys(['descriptive', 'explanatory', 'counterfactual']))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example['ans_dict'], example['ques_dict'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 17, 8, 3, 2, 7, 19, 11, 1, 1, 7]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example['ans_dict']['descriptive'].detach().to('cpu').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'argmax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18108/2473253648.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_18108/1509174066.py\u001b[0m in \u001b[0;36mce_pred\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mce_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mce_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'argmax'"
     ]
    }
   ],
   "source": [
    "a(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
