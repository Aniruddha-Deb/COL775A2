{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4054a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sit/phd/anz228400/.conda/envs/dl/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torchvision\n",
    "# from skimage import io\n",
    "import cv2\n",
    "\n",
    "import copy, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5525dda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "options = ['0', '1', '2', '3', '4', '5', 'yes', 'no', 'rubber', 'metal', 'sphere', 'cube', 'cylinder', 'gray', 'brown', 'green', 'red', 'blue', 'purple', 'yellow', 'cyan']\n",
    "option_id_map = {\n",
    "    o:i for i,o in enumerate(options)\n",
    "}\n",
    "id_option_map = {\n",
    "    i:o for i,o in enumerate(options)\n",
    "}\n",
    "task_heads = ['descriptive', 'explanatory', 'predictive', 'counterfactual']\n",
    "binary_id_map = {'wrong': 0, 'correct': 1}\n",
    "id_binary_map = {0: 'wrong', 1: 'correct'}\n",
    "ques_dict_keys = [\"tokens\", \"q_ids\", \"choice_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7db25aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessQuestions:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        pass\n",
    "        \n",
    "    def get_qa_batch(self, ques_list):\n",
    "        '''\n",
    "        INPUT:\n",
    "        ques_list: list of question_data dictionary\n",
    "        OUTPUT: question_dict, answer_dict\n",
    "        descriptive: \n",
    "            question_list: list of <question> [SEP] <question_subtype>\n",
    "            answer_list: list of respective answer as option_id_map\n",
    "        explanatory:\n",
    "            question_list: list of <question> [SEP] <choice_k>\n",
    "            answer_list: list of respective answer as binary_id_map correct = 1 / wrong = 0\n",
    "        predictive:\n",
    "            question_list: list of <question> [SEP] <choice_k>\n",
    "            answer_list: list of respective answer as binary_id_map correct = 1 / wrong = 0\n",
    "        counterfactual:\n",
    "            question_list: list of <question> [SEP] <choice_k>\n",
    "            answer_list: list of respective answer as binary_id_map correct = 1 / wrong = 0\n",
    "        '''\n",
    "        \n",
    "        question_dict = {i:{j:[] for j in ques_dict_keys} for i in task_heads}\n",
    "        answer_dict = {i:[] for i in task_heads}\n",
    "        for j, q in enumerate(ques_list):\n",
    "            question_type = q['question_type']\n",
    "            \n",
    "            if question_type == \"descriptive\":\n",
    "                question_dict[question_type]['tokens'].append(q['question'] + \" [SEP] \" + q['question_subtype'])\n",
    "                question_dict[question_type]['q_ids'].append(q['question_id'])\n",
    "                answer_dict[question_type].append(option_id_map[q['answer']])\n",
    "\n",
    "            elif question_type == \"explanatory\":                \n",
    "                question = q['question']\n",
    "                q_id = q['question_id']                \n",
    "                for c, choice in enumerate(q['choices']):\n",
    "                    question_dict[question_type]['tokens'].append(question + \" [SEP] \" + choice['choice'])\n",
    "                    question_dict[question_type]['q_ids'].append(q_id)\n",
    "                    question_dict[question_type]['choice_ids'].append(choice['choice_id'])\n",
    "                    answer_dict[question_type].append(binary_id_map[choice['answer']])\n",
    "\n",
    "            elif question_type == \"predictive\":               \n",
    "                question = q['question']\n",
    "                q_id = q['question_id']                \n",
    "                for c, choice in enumerate(q['choices']):\n",
    "                    question_dict[question_type]['tokens'].append(question + \" [SEP] \" + choice['choice'])\n",
    "                    question_dict[question_type]['q_ids'].append(q_id)\n",
    "                    question_dict[question_type]['choice_ids'].append(choice['choice_id'])\n",
    "                    answer_dict[question_type].append(binary_id_map[choice['answer']])\n",
    "      \n",
    "            elif question_type == \"counterfactual\":               \n",
    "                question = q['question']\n",
    "                q_id = q['question_id']                \n",
    "                for c, choice in enumerate(q['choices']):\n",
    "                    question_dict[question_type]['tokens'].append(question + \" [SEP] \" + choice['choice'])\n",
    "                    question_dict[question_type]['q_ids'].append(q_id)\n",
    "                    question_dict[question_type]['choice_ids'].append(choice['choice_id'])\n",
    "                    answer_dict[question_type].append(binary_id_map[choice['answer']])\n",
    "        \n",
    "        for th in task_heads:\n",
    "            if answer_dict[th]:\n",
    "                question_dict[th]['tokens'] = self.tokenizer(question_dict[th]['tokens'], return_tensors='pt', padding=True)\n",
    "                question_dict[th]['q_ids'] = torch.tensor(question_dict[th]['q_ids'], dtype=torch.long)\n",
    "                question_dict[th]['choice_ids'] = torch.tensor(question_dict[th]['choice_ids'], dtype=torch.long)\n",
    "                answer_dict[th] = torch.tensor(answer_dict[th], dtype=torch.long)\n",
    "                \n",
    "                if th != 'descriptive':\n",
    "                    answer_dict[th] = answer_dict[th].float()\n",
    "            else:\n",
    "                del question_dict[th]\n",
    "                del answer_dict[th]\n",
    "        \n",
    "        return question_dict, answer_dict\n",
    "        \n",
    "    \n",
    "class CLEVRERDataset(Dataset):\n",
    "    def __init__(self, data_dir, frame_dir, tokenizer):\n",
    "        # TODO load annotations\n",
    "        assert os.path.isdir(data_dir)\n",
    "        assert os.path.isdir(frame_dir)\n",
    "        \n",
    "        with open(os.path.join(data_dir, data_dir.split(\"/\")[-1] + \".json\"), \"r\") as f:\n",
    "            self.json_data = json.load(f)\n",
    "        self.frame_dir = frame_dir\n",
    "        \n",
    "        self.process_questions = ProcessQuestions(tokenizer)\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        # get length from directory\n",
    "        return len(self.json_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "        1. Change here hardcoded path in frame_paths to os.path.join(self.frame_dir, f\"sim_{vid_id}\", \"*.png\")\n",
    "        2. Check normalization mean and std values used in image transform.\n",
    "        \"\"\"\n",
    "        \n",
    "        vid_json = self.json_data[idx]\n",
    "        vid_id = vid_json['scene_index']\n",
    "        \n",
    "        frame_dir = os.path.join(self.frame_dir, f\"sim_{vid_id:05d}\", \"*.png\")\n",
    "        frame_paths = sorted(glob(frame_dir))\n",
    "        frames = torch.stack([torchvision.io.read_image(img).float() for img in frame_paths[::5]])\n",
    "                \n",
    "        ques_dict, ans_dict = self.process_questions.get_qa_batch(vid_json['questions'])\n",
    "\n",
    "        return {'scene_index': vid_id, 'frames': frames, 'ques_dict': ques_dict, 'ans_dict': ans_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "271e5c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DescriptiveTaskHead(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, n_classes=21, p=0.2, input_dim=768*2):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.clf = nn.Sequential(\n",
    "\t\t\tnn.Linear(input_dim, 1024),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.Linear(1024, n_classes)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, features):\n",
    "\t\treturn self.clf(features)\n",
    "\n",
    "class ExplanatoryTaskHead(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, p=0.2, input_dim=768*2):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.clf = nn.Sequential(\n",
    "\t\t\tnn.Linear(input_dim, 1024),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.Linear(1024, 1),\n",
    "            nn.Sigmoid()\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, features):\n",
    "\t\treturn self.clf(features).reshape(-1)\n",
    "\n",
    "class PredictiveTaskHead(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, p=0.2, input_dim=768*2):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.clf = nn.Sequential(\n",
    "\t\t\tnn.Linear(input_dim, 1024),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.Linear(1024, 1),\n",
    "            nn.Sigmoid()\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, features):\n",
    "\t\treturn self.clf(features).reshape(-1)\n",
    "\n",
    "class CounterfactualTaskHead(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, p=0.2, input_dim=768*2):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.clf = nn.Sequential(\n",
    "\t\t\tnn.Linear(input_dim, 1024),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(p=0.2),\n",
    "\t\t\tnn.Linear(1024, 1),\n",
    "            nn.Sigmoid()\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, features):\n",
    "\t\treturn self.clf(features).reshape(-1)\n",
    "\n",
    "class BertCNNModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size=768):\n",
    "        super().__init__()\n",
    "        self.cnn = torchvision.models.resnet50(pretrained=True)\n",
    "        self.cnn.fc = nn.Identity()\n",
    "        \n",
    "        for name, param in self.cnn.named_parameters():\n",
    "            if not name.startswith('layer4'):\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "                input_size=2048,\n",
    "                batch_first=True,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=1\n",
    "            )\n",
    "        self.bert = transformers.BertModel.from_pretrained('bert-base-cased')\n",
    "        \n",
    "        self.h0 = nn.Parameter(torch.empty(1,hidden_size).normal_(0, 0.1))\n",
    "        self.c0 = nn.Parameter(torch.empty(1,hidden_size).normal_(0, 0.1))\n",
    "        \n",
    "        self.descriptive_head = DescriptiveTaskHead()\n",
    "        self.explanatory_head = ExplanatoryTaskHead()\n",
    "        self.predictive_head = PredictiveTaskHead()\n",
    "        self.counterfactual_head = CounterfactualTaskHead()\n",
    "        \n",
    "        self.head_map = {\n",
    "            'descriptive': self.descriptive_head,\n",
    "            'predictive': self.predictive_head,\n",
    "            'explanatory': self.explanatory_head,\n",
    "            'counterfactual': self.counterfactual_head\n",
    "        }\n",
    "        \n",
    "    def forward(self, example):\n",
    "        \n",
    "        N, C, H, W = example['frames'].shape\n",
    "        # i = 0\n",
    "        # bs = 8\n",
    "        # frame_emb = []\n",
    "        # while (i*bs < N):\n",
    "        #     frame_emb += [self.cnn(example['frames'][i*bs:(i+1)*bs])]\n",
    "        #     i += 1\n",
    "        #     \n",
    "        # frame_emb = torch.vstack(frame_emb)\n",
    "        frame_emb = self.cnn(example['frames'])\n",
    "        frame_encs, (video_enc, last_cell_state) = self.lstm(frame_emb, (self.h0, self.c0))\n",
    "        \n",
    "        # faster to batch everything and send, but this works for now\n",
    "        preds = {}\n",
    "        for task, ques_data in example['ques_dict'].items():\n",
    "\n",
    "            bert_output = self.bert(**ques_data['tokens'])\n",
    "\n",
    "            # feature vector\n",
    "            features = torch.hstack([video_enc.repeat(bert_output.pooler_output.size(0),1), bert_output.pooler_output])\n",
    "\n",
    "            preds[task] = self.head_map[task](features)\n",
    "        \n",
    "        return preds\n",
    "\n",
    "\n",
    "def dl_collate_fn(data):\n",
    "    return data[0]\n",
    "\n",
    "def ques_to_device(d):\n",
    "    return {k: {k_dash: v_dash.to(device) for k_dash, v_dash in v.items()} for k,v in d.items()}\n",
    "\n",
    "def ans_to_device(d):\n",
    "    return {k: v.to(device) for k,v in d.items()}\n",
    "\n",
    "def process_example(example, transform):\n",
    "    return {\n",
    "        'frames': transform(example['frames'].to(device)),\n",
    "        'ques_dict': ques_to_device(example['ques_dict']),\n",
    "        'ans_dict': ans_to_device(example['ans_dict'])\n",
    "    }\n",
    "\n",
    "def load_checkpoint(file_path):\n",
    "\n",
    "    print(\"loading model:\", file_path)\n",
    "    model = torch.load(file_path).to(device)\n",
    "    print(\"model load successful...\")\n",
    "\n",
    "    return model\n",
    "\n",
    "class MULTICALSS_PREDS:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def get_pred(self, output):\n",
    "        return output.argmax(dim=1)\n",
    "\n",
    "class BINARY_PREDS:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def get_pred(self, output):\n",
    "        return output.round()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687b3dee",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9326bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(model, eval_dl, output_prefix=None):\n",
    "    \n",
    "    pred_fns = {\n",
    "        'descriptive': MULTICALSS_PREDS(),\n",
    "        'predictive': BINARY_PREDS(),\n",
    "        'explanatory': BINARY_PREDS(),\n",
    "        'counterfactual': BINARY_PREDS()\n",
    "    }\n",
    "    \n",
    "    pred_dict = {k: [] for k in task_heads}\n",
    "    gold_dict = {k: [] for k in task_heads}\n",
    "    predictions_json = []\n",
    "    accuracy_dict = {k: -1 for k in task_heads}\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for example in tqdm(eval_dl):\n",
    "            scene_pred = {'scene_index': example['scene_index'], 'questions': []}\n",
    "            \n",
    "            example = process_example(example, img_transform)\n",
    "\n",
    "            outputs = model(example)\n",
    "            \n",
    "            for task, output in outputs.items():\n",
    "                pred = pred_fns[task].get_pred(output).detach().to('cpu').tolist()\n",
    "                gold = example['ans_dict'][task].detach().to('cpu').tolist()\n",
    "                pred_dict[task].extend(pred)\n",
    "                gold_dict[task].extend(gold)\n",
    "                \n",
    "                \n",
    "                if task == 'descriptive':\n",
    "                    q_ids = example['ques_dict'][task]['q_ids'].detach().to('cpu').tolist()\n",
    "                    for i in range(len(q_ids)):\n",
    "                        scene_pred['questions'].append({\"question_id\" : q_ids[i], \"answer\": id_option_map[pred[i]]})\n",
    "                \n",
    "                else:\n",
    "                    q_ids = example['ques_dict'][task]['q_ids']\n",
    "                    assert len(q_ids) == len(pred)\n",
    "\n",
    "                    choice_ids = example['ques_dict'][task]['choice_ids'].detach().to('cpu').tolist()\n",
    "                    unique_q_ids = q_ids.unique().detach().to('cpu').tolist()\n",
    "                    temp_ques_list = [{\"question_id\": q, \"choices\": []} for q in unique_q_ids]\n",
    "\n",
    "                    for i, q in enumerate(q_ids):\n",
    "                        temp_ques_list[unique_q_ids.index(q)][\"choices\"].append({\"choice_id\": choice_ids[i], \"answer\": id_binary_map[pred[i]]})\n",
    "\n",
    "                    scene_pred['questions'].extend(temp_ques_list)\n",
    "\n",
    "            predictions_json.append(scene_pred)            \n",
    "                \n",
    "                    \n",
    "                     \n",
    "                \n",
    "    for th in task_heads:\n",
    "        accuracy_dict[th] = accuracy_score(gold_dict[th], pred_dict[th])\n",
    "    \n",
    "    with open(f\"{output_prefix}-predictions.json\", \"w+\") as file:\n",
    "        json.dump(predictions_json, file)\n",
    "\n",
    "    with open(f\"{output_prefix}-accuracy.json\", \"w+\") as file:\n",
    "        json.dump(accuracy_dict, file)\n",
    "         \n",
    "    with open(f\"{output_prefix}-gold.json\", \"w+\") as file:\n",
    "        json.dump(gold_dict, file)\n",
    "    \n",
    "    with open(f\"{output_prefix}-pred.json\", \"w+\") as file:\n",
    "        json.dump(pred_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb43e731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model: ../models/baseline-5.pt\n",
      "model load successful...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████| 8/8 [00:02<00:00,  3.29it/s]\n"
     ]
    }
   ],
   "source": [
    "img_transform = torchvision.transforms.Compose([torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                                                 (0.2023, 0.1994, 0.2010))])\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "eval_ds = CLEVRERDataset(\"../../../data/data/validation\", \"../../../clevrer_code/frames\", tokenizer)\n",
    "\n",
    "DEBUG = True\n",
    "if DEBUG:\n",
    "    eval_ds.json_data = eval_ds.json_data[:8]\n",
    "\n",
    "eval_dl = DataLoader(eval_ds, batch_size=1, collate_fn=dl_collate_fn, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "model_file = '../models/baseline-5.pt'\n",
    "\n",
    "model = load_checkpoint(model_file)\n",
    "\n",
    "model_eval(model, eval_dl, f'../results/{model_file.split(\"/\")[-1].split(\".\")[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a17e6f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'baseline-5'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_file = '../models/baseline-5.pt'\n",
    "model_file.split(\"/\")[-1].split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102167fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9220b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 0., 0., 0., 1., 1., 0., 1.], device='cuda:0'),\n",
       " tensor([0., 0., 1., 1., 1., 1., 0., 1.], device='cuda:0'))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th = 0.5\n",
    "\n",
    "outputs['explanatory'].round(), example['ans_dict']['explanatory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae448fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'descriptive': tensor([10, 17,  8,  3,  2,  7, 19, 11,  1,  1,  7], device='cuda:0'),\n",
       "  'explanatory': tensor([0., 0., 1., 1., 1., 1., 0., 1.], device='cuda:0'),\n",
       "  'counterfactual': tensor([0., 0., 0., 1., 0., 1., 1., 0.], device='cuda:0')},\n",
       " dict_keys(['descriptive', 'explanatory', 'counterfactual']))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example['ans_dict'], example['ques_dict'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "86ab6a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 17, 8, 3, 2, 7, 19, 11, 1, 1, 7]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example['ans_dict']['descriptive'].detach().to('cpu').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "55e82b98",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'argmax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18108/2473253648.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_18108/1509174066.py\u001b[0m in \u001b[0;36mce_pred\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mce_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mce_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'argmax'"
     ]
    }
   ],
   "source": [
    "a(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5113167",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
